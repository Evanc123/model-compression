{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: Yeilds the same training policy when the cross entropies are weighted against each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.8\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.06\n",
      "step 400, training accuracy 0.08\n",
      "step 500, training accuracy 0.08\n",
      "step 600, training accuracy 0.06\n",
      "step 700, training accuracy 0.14\n",
      "step 800, training accuracy 0.06\n",
      "step 900, training accuracy 0.1\n",
      "step 1000, training accuracy 0.14\n",
      "step 1100, training accuracy 0.08\n",
      "step 1200, training accuracy 0.1\n",
      "step 1300, training accuracy 0.08\n",
      "step 1400, training accuracy 0.12\n",
      "step 1500, training accuracy 0.06\n",
      "step 1600, training accuracy 0.06\n",
      "step 1700, training accuracy 0.08\n",
      "step 1800, training accuracy 0.1\n",
      "step 1900, training accuracy 0.12\n",
      "test accuracy 0.098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def weight_variable(shape):\n",
    "\tinitial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "\tinitial = tf.constant(.1, shape=shape)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "\treturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\treturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \n",
    "\t\t\t\t\t\t\tstrides = [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "y_expert = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "\n",
    "probs = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "temperature = 10\n",
    "t_probs = tf.nn.softmax(tf.div(probs, temperature))\n",
    "y_conv = tf.nn.softmax(probs)\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "cross_entropy_expert = tf.reduce_mean(-tf.reduce_sum(y_expert * tf.log(t_probs), reduction_indices=[1])) \n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(.2 * cross_entropy + .8 * cross_entropy_expert )\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(2000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], y_expert: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], y_expert: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.1\n",
      "step 100, training accuracy 0.84\n",
      "step 200, training accuracy 0.1\n",
      "step 300, training accuracy 0.06\n",
      "step 400, training accuracy 0.08\n",
      "step 500, training accuracy 0.08\n",
      "step 600, training accuracy 0.06\n",
      "step 700, training accuracy 0.14\n",
      "step 800, training accuracy 0.06\n",
      "step 900, training accuracy 0.1\n",
      "step 1000, training accuracy 0.14\n",
      "step 1100, training accuracy 0.1\n",
      "step 1200, training accuracy 0.08\n",
      "step 1300, training accuracy 0.1\n",
      "step 1400, training accuracy 0.12\n",
      "step 1500, training accuracy 0.12\n",
      "step 1600, training accuracy 0.04\n",
      "step 1700, training accuracy 0.08\n",
      "step 1800, training accuracy 0.06\n",
      "step 1900, training accuracy 0.08\n",
      "test accuracy 0.098\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tensorflow\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def replace_none_with_zero(l):\n",
    "    return [0 if i==None else i for i in l]\n",
    "\n",
    "def weight_variable(shape):\n",
    "\tinitial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "\tinitial = tf.constant(.1, shape=shape)\n",
    "\treturn tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "\treturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\treturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1], \n",
    "\t\t\t\t\t\t\tstrides = [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "y_expert = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "\n",
    "probs = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "temperature = 10.0\n",
    "temp_square = temperature * temperature \n",
    "t_probs = tf.nn.softmax(tf.div(probs, temperature))\n",
    "y_conv = tf.nn.softmax(probs)\n",
    "\n",
    "\n",
    "cross_entropy = .9* tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "cross_entropy_expert = .1 * tf.reduce_mean(-tf.reduce_sum(y_expert * tf.log(t_probs), reduction_indices=[1])) \n",
    "optim = tf.train.AdamOptimizer(1e-4)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grads_and_vars = optim.compute_gradients(cross_entropy)\n",
    "\n",
    "\n",
    "temp_grads = replace_none_with_zero(optim.compute_gradients(cross_entropy_expert))\n",
    "\n",
    "temp_compensated_grads = [(grad * temp_square, var) for grad, var in temp_grads]\n",
    "\n",
    "\n",
    "\n",
    "#train_op = optim.apply_gradients(grads_and_vars + grads_and_vars_two)\n",
    "\n",
    "train_op = optim.apply_gradients(temp_compensated_grads + grads_and_vars)\n",
    "\n",
    "#grads_and_vars_soft = optim.compute_gradients(cross_entropy_expert)\n",
    "\n",
    "#grads_and_vars_hard = optim.compute_gradients(cross_entropy)\n",
    "\n",
    "sess.run(tf.initialize_all_variables()) \n",
    "\n",
    "\n",
    "#sess.run(optim.apply_gradients(grads_and_vars_hard + grads_and_vars_soft))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%100 == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], y_expert: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "  train_op.run(feed_dict={x: batch[0], y_: batch[1], y_expert: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
